In our approach the main steps are as follows. A model of a training object is presented in a rigid body physics simulator. Then a number of example grasps are executed by a human, with the precise motions of hand and object during contact being determined by the simulation. Each example grasp continues until a final stable grasp state is reached. We call this the {\em equilibrium state}, consisting of the final hand shape, and the final set of contact relations between hand and object. For training and inference purposes each example grasp has two parts: an equilibrium state, and the reach to grasp trajectory leading to it.

We generate the example grasps in sets. Each set corresponds to a type of grasp, e.g. power or pinch. This means that the equilibrium states are similar within a set, but differ substantially between sets. 

Models are then learned for each grasp and for each set. Models are learned of the reach to grasp, the hand configuration in the equilibrium state, and the contact relations between hand and object in the equilibrium state. Given these models, when a new object is presented, a (partial) model of that object is obtained by sensing. This model is combined with the models learned from the training grasps.

Then many candidate equilibrium states, and associated candidate reach to grasp trajectories are generated by sampling. Finally they are optimised so as to maximise the likelihood of the grasp according to a product of experts.